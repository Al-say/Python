# ç°ä»£åŒ–æ‹›è˜ä¿¡æ¯çˆ¬è™«ç³»ç»Ÿ
# ä½¿ç”¨å¼‚æ­¥æŠ€æœ¯ã€åçˆ¬è™«ç­–ç•¥å’Œæ•°æ®å¯è§†åŒ–

import asyncio
import aiohttp
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import re
import time
import random
from urllib.parse import urljoin, urlparse
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import sqlite3
from typing import List, Dict, Optional
import logging
from fake_useragent import UserAgent
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class JobSpider:
    """ç°ä»£åŒ–æ‹›è˜ä¿¡æ¯çˆ¬è™«"""

    def __init__(self, db_path='job_data.db'):
        self.db_path = db_path
        self.ua = UserAgent()
        self.session = None
        self.driver = None
        self.init_database()

        # åçˆ¬è™«ç­–ç•¥
        self.request_delay = (1, 3)  # è¯·æ±‚é—´éš”1-3ç§’
        self.max_retries = 3
        self.timeout = 30

        # ç›®æ ‡ç½‘ç«™é…ç½®
        self.sources = {
            'lagou': {
                'name': 'æ‹‰å‹¾ç½‘',
                'base_url': 'https://www.lagou.com',
                'search_url': 'https://www.lagou.com/wn/jobs?pn={page}&kd={keyword}',
                'parser': self.parse_lagou
            },
            'boss': {
                'name': 'Bossç›´è˜',
                'base_url': 'https://www.zhipin.com',
                'search_url': 'https://www.zhipin.com/web/geek/job?query={keyword}&page={page}',
                'parser': self.parse_boss,
                'use_selenium': True
            },
            'bilibili': {
                'name': 'Bilibiliæ‹›è˜',
                'base_url': 'https://jobs.bilibili.com',
                'search_url': 'https://jobs.bilibili.com/search?keyword={keyword}&page={page}',
                'parser': self.parse_bilibili
            }
        }

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # åˆ›å»ºæ‹›è˜ä¿¡æ¯è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS jobs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                job_id TEXT UNIQUE,
                title TEXT NOT NULL,
                company TEXT NOT NULL,
                salary TEXT,
                location TEXT,
                experience TEXT,
                education TEXT,
                description TEXT,
                tags TEXT,  -- JSONæ ¼å¼å­˜å‚¨æ ‡ç­¾
                source TEXT,
                url TEXT,
                publish_time TEXT,
                crawl_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                status TEXT DEFAULT 'active'
            )
        ''')

        # åˆ›å»ºå…¬å¸ä¿¡æ¯è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS companies (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE,
                industry TEXT,
                size TEXT,
                description TEXT,
                website TEXT,
                logo_url TEXT,
                update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        # åˆ›å»ºæœç´¢å…³é”®è¯è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS search_keywords (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                keyword TEXT UNIQUE,
                search_count INTEGER DEFAULT 0,
                last_search TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        conn.commit()
        conn.close()
        logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

    def init_session(self):
        """åˆå§‹åŒ–å¼‚æ­¥ä¼šè¯"""
        if not self.session:
            self.session = aiohttp.ClientSession(
                headers={
                    'User-Agent': self.ua.random,
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                }
            )

    def init_selenium_driver(self):
        """åˆå§‹åŒ–Seleniumæµè§ˆå™¨"""
        if not self.driver:
            options = Options()
            options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option('useAutomationExtension', False)
            options.add_argument(f'--user-agent={self.ua.random}')

            self.driver = webdriver.Chrome(options=options)
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    async def close(self):
        """å…³é—­èµ„æº"""
        if self.session:
            await self.session.close()
        if self.driver:
            self.driver.quit()

    def save_job(self, job_data: Dict):
        """ä¿å­˜æ‹›è˜ä¿¡æ¯åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        try:
            cursor.execute('''
                INSERT OR REPLACE INTO jobs
                (job_id, title, company, salary, location, experience, education,
                 description, tags, source, url, publish_time)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                job_data.get('job_id'),
                job_data.get('title'),
                job_data.get('company'),
                job_data.get('salary'),
                job_data.get('location'),
                job_data.get('experience'),
                job_data.get('education'),
                job_data.get('description'),
                json.dumps(job_data.get('tags', [])),
                job_data.get('source'),
                job_data.get('url'),
                job_data.get('publish_time')
            ))
            conn.commit()
            logger.info(f"âœ… ä¿å­˜èŒä½: {job_data.get('title')} - {job_data.get('company')}")

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜èŒä½å¤±è´¥: {e}")
        finally:
            conn.close()

    async def fetch_page(self, url: str, use_selenium: bool = False) -> Optional[str]:
        """è·å–é¡µé¢å†…å®¹"""
        for attempt in range(self.max_retries):
            try:
                if use_selenium:
                    if not self.driver:
                        self.init_selenium_driver()
                    self.driver.get(url)
                    WebDriverWait(self.driver, 10).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )
                    return self.driver.page_source
                else:
                    if not self.session:
                        self.init_session()
                    async with self.session.get(url, timeout=self.timeout) as response:
                        if response.status == 200:
                            return await response.text()
                        else:
                            logger.warning(f"è¯·æ±‚å¤±è´¥: {url} - çŠ¶æ€ç : {response.status}")

            except Exception as e:
                logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {e}")

            # éšæœºå»¶è¿Ÿ
            await asyncio.sleep(random.uniform(*self.request_delay))

        return None

    def parse_lagou(self, html: str, source_config: Dict) -> List[Dict]:
        """è§£ææ‹‰å‹¾ç½‘æ‹›è˜ä¿¡æ¯"""
        soup = BeautifulSoup(html, 'html.parser')
        jobs = []

        # æ‹‰å‹¾ç½‘çš„èŒä½åˆ—è¡¨é€‰æ‹©å™¨ï¼ˆå¯èƒ½éœ€è¦æ ¹æ®å®é™…é¡µé¢è°ƒæ•´ï¼‰
        job_items = soup.select('.job-list .job-item') or soup.select('[data-jobid]')

        for item in job_items[:10]:  # é™åˆ¶æ•°é‡é¿å…è¢«é™åˆ¶
            try:
                job = {
                    'job_id': f"lagou_{item.get('data-jobid', str(hash(str(item))))}",
                    'title': item.select_one('.job-name, .position-link h3').text.strip() if item.select_one('.job-name, .position-link h3') else '',
                    'company': item.select_one('.company-name, .company').text.strip() if item.select_one('.company-name, .company') else '',
                    'salary': item.select_one('.salary, .money').text.strip() if item.select_one('.salary, .money') else '',
                    'location': item.select_one('.job-area, .area').text.strip() if item.select_one('.job-area, .area') else '',
                    'experience': item.select_one('.experience').text.strip() if item.select_one('.experience') else '',
                    'education': item.select_one('.education').text.strip() if item.select_one('.education') else '',
                    'description': item.select_one('.job-desc, .description').text.strip() if item.select_one('.job-desc, .description') else '',
                    'tags': [tag.text.strip() for tag in item.select('.tags span, .labels span')],
                    'source': 'æ‹‰å‹¾ç½‘',
                    'url': urljoin(source_config['base_url'], item.select_one('a')['href']) if item.select_one('a') else '',
                    'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                if job['title'] and job['company']:
                    jobs.append(job)
            except Exception as e:
                logger.warning(f"è§£ææ‹‰å‹¾ç½‘èŒä½å¤±è´¥: {e}")
                continue

        return jobs

    def parse_boss(self, html: str, source_config: Dict) -> List[Dict]:
        """è§£æBossç›´è˜æ‹›è˜ä¿¡æ¯"""
        soup = BeautifulSoup(html, 'html.parser')
        jobs = []

        # Bossç›´è˜çš„èŒä½åˆ—è¡¨é€‰æ‹©å™¨
        job_items = soup.select('.job-card-wrapper, .job-list-item')

        for item in job_items[:10]:
            try:
                job = {
                    'job_id': f"boss_{item.get('data-jobid', str(hash(str(item))))}",
                    'title': item.select_one('.job-name, .job-title').text.strip() if item.select_one('.job-name, .job-title') else '',
                    'company': item.select_one('.company-name, .company-text').text.strip() if item.select_one('.company-name, .company-text') else '',
                    'salary': item.select_one('.salary, .money').text.strip() if item.select_one('.salary, .money') else '',
                    'location': item.select_one('.job-area, .area').text.strip() if item.select_one('.job-area, .area') else '',
                    'experience': item.select_one('.job-experience, .experience').text.strip() if item.select_one('.job-experience, .experience') else '',
                    'education': item.select_one('.job-education, .education').text.strip() if item.select_one('.job-education, .education') else '',
                    'description': item.select_one('.job-desc, .description').text.strip() if item.select_one('.job-desc, .description') else '',
                    'tags': [tag.text.strip() for tag in item.select('.tag, .labels span')],
                    'source': 'Bossç›´è˜',
                    'url': urljoin(source_config['base_url'], item.select_one('a')['href']) if item.select_one('a') else '',
                    'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                if job['title'] and job['company']:
                    jobs.append(job)
            except Exception as e:
                logger.warning(f"è§£æBossç›´è˜èŒä½å¤±è´¥: {e}")
                continue

        return jobs

    def parse_bilibili(self, html: str, source_config: Dict) -> List[Dict]:
        """è§£æBilibiliæ‹›è˜ä¿¡æ¯"""
        soup = BeautifulSoup(html, 'html.parser')
        jobs = []

        # Bilibiliæ‹›è˜çš„èŒä½åˆ—è¡¨é€‰æ‹©å™¨
        job_items = soup.select('.job-item, .position-item')

        for item in job_items[:10]:
            try:
                job = {
                    'job_id': f"bilibili_{item.get('data-jobid', str(hash(str(item))))}",
                    'title': item.select_one('.job-title, .position-title').text.strip() if item.select_one('.job-title, .position-title') else '',
                    'company': 'å“”å“©å“”å“©',  # Bç«™æ‹›è˜é€šå¸¸æ˜¯å†…éƒ¨æ‹›è˜
                    'salary': item.select_one('.salary, .money').text.strip() if item.select_one('.salary, .money') else '',
                    'location': item.select_one('.location, .area').text.strip() if item.select_one('.location, .area') else '',
                    'experience': item.select_one('.experience').text.strip() if item.select_one('.experience') else '',
                    'education': item.select_one('.education').text.strip() if item.select_one('.education') else '',
                    'description': item.select_one('.description, .job-desc').text.strip() if item.select_one('.description, .job-desc') else '',
                    'tags': [tag.text.strip() for tag in item.select('.tag, .label')],
                    'source': 'Bilibiliæ‹›è˜',
                    'url': urljoin(source_config['base_url'], item.select_one('a')['href']) if item.select_one('a') else '',
                    'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                if job['title']:
                    jobs.append(job)
            except Exception as e:
                logger.warning(f"è§£æBilibiliæ‹›è˜èŒä½å¤±è´¥: {e}")
                continue

        return jobs

    async def crawl_source(self, source_name: str, keyword: str, max_pages: int = 3) -> List[Dict]:
        """çˆ¬å–å•ä¸ªæ•°æ®æº"""
        source_config = self.sources.get(source_name)
        if not source_config:
            logger.error(f"ä¸æ”¯æŒçš„æ•°æ®æº: {source_name}")
            return []

        all_jobs = []
        logger.info(f"å¼€å§‹çˆ¬å– {source_config['name']} - å…³é”®è¯: {keyword}")

        for page in range(1, max_pages + 1):
            try:
                url = source_config['search_url'].format(keyword=keyword, page=page)
                logger.info(f"çˆ¬å–ç¬¬ {page} é¡µ: {url}")

                html = await self.fetch_page(url, source_config.get('use_selenium', False))
                if not html:
                    logger.warning(f"è·å–é¡µé¢å¤±è´¥: {url}")
                    continue

                jobs = source_config['parser'](html, source_config)
                all_jobs.extend(jobs)

                logger.info(f"ç¬¬ {page} é¡µè·å–åˆ° {len(jobs)} ä¸ªèŒä½")

                # é¡µé¢é—´å»¶è¿Ÿ
                await asyncio.sleep(random.uniform(2, 5))

            except Exception as e:
                logger.error(f"çˆ¬å–ç¬¬ {page} é¡µå¤±è´¥: {e}")
                continue

        return all_jobs

    async def crawl_all_sources(self, keyword: str, max_pages: int = 2) -> List[Dict]:
        """å¹¶å‘çˆ¬å–æ‰€æœ‰æ•°æ®æº"""
        logger.info(f"å¼€å§‹å¹¶å‘çˆ¬å–æ‰€æœ‰æ•°æ®æº - å…³é”®è¯: {keyword}")

        tasks = []
        for source_name in self.sources.keys():
            task = self.crawl_source(source_name, keyword, max_pages)
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)

        all_jobs = []
        for i, result in enumerate(results):
            source_name = list(self.sources.keys())[i]
            if isinstance(result, Exception):
                logger.error(f"çˆ¬å– {source_name} å¤±è´¥: {result}")
            else:
                all_jobs.extend(result)
                logger.info(f"{source_name} å…±è·å– {len(result)} ä¸ªèŒä½")

        return all_jobs

    def analyze_jobs(self, jobs: List[Dict]) -> Dict:
        """åˆ†ææ‹›è˜æ•°æ®"""
        if not jobs:
            return {}

        df = pd.DataFrame(jobs)

        analysis = {
            'total_jobs': len(df),
            'unique_companies': df['company'].nunique(),
            'sources': df['source'].value_counts().to_dict(),
            'locations': df['location'].value_counts().head(10).to_dict(),
            'salary_ranges': self.analyze_salary(df),
            'experience_distribution': df['experience'].value_counts().to_dict() if 'experience' in df.columns else {},
            'education_distribution': df['education'].value_counts().to_dict() if 'education' in df.columns else {},
            'top_companies': df['company'].value_counts().head(10).to_dict(),
            'common_tags': self.analyze_tags(df)
        }

        return analysis

    def analyze_salary(self, df: pd.DataFrame) -> Dict:
        """åˆ†æè–ªèµ„åˆ†å¸ƒ"""
        salary_ranges = defaultdict(int)

        for salary in df['salary'].dropna():
            # ç®€å•çš„è–ªèµ„èŒƒå›´è¯†åˆ«ï¼ˆå¯ä»¥æ ¹æ®å®é™…æ•°æ®è°ƒæ•´ï¼‰
            if 'k' in salary.lower() or 'åƒ' in salary:
                if '-' in salary:
                    parts = salary.replace('k', '').replace('åƒ', '').split('-')
                    try:
                        min_salary = float(parts[0].strip())
                        max_salary = float(parts[1].strip())
                        if max_salary <= 20:
                            salary_ranges['0-20k'] += 1
                        elif max_salary <= 50:
                            salary_ranges['20-50k'] += 1
                        else:
                            salary_ranges['50k+'] += 1
                    except:
                        salary_ranges['æœªçŸ¥'] += 1
                else:
                    salary_ranges['é¢è®®/æœªçŸ¥'] += 1
            else:
                salary_ranges['é¢è®®/æœªçŸ¥'] += 1

        return dict(salary_ranges)

    def analyze_tags(self, df: pd.DataFrame) -> Dict:
        """åˆ†æèŒä½æ ‡ç­¾"""
        all_tags = []
        for tags in df['tags'].dropna():
            if isinstance(tags, str):
                try:
                    tag_list = json.loads(tags)
                    all_tags.extend(tag_list)
                except:
                    continue
            elif isinstance(tags, list):
                all_tags.extend(tags)

        tag_counts = pd.Series(all_tags).value_counts().head(20)
        return tag_counts.to_dict()

    def visualize_analysis(self, analysis: Dict, keyword: str):
        """å¯è§†åŒ–åˆ†æç»“æœ"""
        if not analysis:
            return

        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'æ‹›è˜æ•°æ®åˆ†æ - å…³é”®è¯: {keyword}', fontsize=16)

        # 1. æ•°æ®æºåˆ†å¸ƒ
        if 'sources' in analysis:
            sources = analysis['sources']
            axes[0, 0].pie(sources.values(), labels=sources.keys(), autopct='%1.1f%%')
            axes[0, 0].set_title('æ•°æ®æºåˆ†å¸ƒ')

        # 2. è–ªèµ„åˆ†å¸ƒ
        if 'salary_ranges' in analysis:
            salary_data = analysis['salary_ranges']
            axes[0, 1].bar(salary_data.keys(), salary_data.values())
            axes[0, 1].set_title('è–ªèµ„åˆ†å¸ƒ')
            axes[0, 1].tick_params(axis='x', rotation=45)

        # 3. å·¥ä½œåœ°ç‚¹åˆ†å¸ƒ
        if 'locations' in analysis:
            locations = analysis['locations']
            axes[1, 0].bar(locations.keys(), locations.values())
            axes[1, 0].set_title('å·¥ä½œåœ°ç‚¹åˆ†å¸ƒ')
            axes[1, 0].tick_params(axis='x', rotation=45)

        # 4. çƒ­é—¨å…¬å¸
        if 'top_companies' in analysis:
            companies = analysis['top_companies']
            axes[1, 1].barh(list(companies.keys()), list(companies.values()))
            axes[1, 1].set_title('çƒ­é—¨å…¬å¸')

        plt.tight_layout()
        plt.savefig(f'job_analysis_{keyword}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png', dpi=300, bbox_inches='tight')
        plt.show()

        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        self.generate_report(analysis, keyword)

    def generate_report(self, analysis: Dict, keyword: str):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = f"""
# æ‹›è˜æ•°æ®åˆ†ææŠ¥å‘Š
## æœç´¢å…³é”®è¯: {keyword}
## ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

### ğŸ“Š åŸºæœ¬ç»Ÿè®¡
- æ€»èŒä½æ•°: {analysis.get('total_jobs', 0)}
- ç‹¬ç‰¹å…¬å¸æ•°: {analysis.get('unique_companies', 0)}

### ğŸ“ æ•°æ®æºåˆ†å¸ƒ
"""
        if 'sources' in analysis:
            for source, count in analysis['sources'].items():
                report += f"- {source}: {count} ä¸ªèŒä½\n"

        report += "\n### ğŸ’° è–ªèµ„åˆ†å¸ƒ\n"
        if 'salary_ranges' in analysis:
            for salary_range, count in analysis['salary_ranges'].items():
                report += f"- {salary_range}: {count} ä¸ªèŒä½\n"

        report += "\n### ğŸ¢ çƒ­é—¨å…¬å¸ TOP 10\n"
        if 'top_companies' in analysis:
            for i, (company, count) in enumerate(analysis['top_companies'].items(), 1):
                report += f"{i}. {company}: {count} ä¸ªèŒä½\n"

        report += "\n### ğŸ“ çƒ­é—¨å·¥ä½œåœ°ç‚¹ TOP 10\n"
        if 'locations' in analysis:
            for i, (location, count) in enumerate(analysis['locations'].items(), 1):
                report += f"{i}. {location}: {count} ä¸ªèŒä½\n"

        # ä¿å­˜æŠ¥å‘Š
        filename = f'job_report_{keyword}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md'
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(report)

        logger.info(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {filename}")

    async def run_crawler(self, keyword: str, max_pages: int = 2):
        """è¿è¡Œå®Œæ•´çš„çˆ¬è™«æµç¨‹"""
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–æ‹›è˜ä¿¡æ¯ - å…³é”®è¯: {keyword}")

        try:
            # 1. å¹¶å‘çˆ¬å–æ‰€æœ‰æ•°æ®æº
            jobs = await self.crawl_all_sources(keyword, max_pages)

            # 2. ä¿å­˜åˆ°æ•°æ®åº“
            for job in jobs:
                self.save_job(job)

            # 3. åˆ†ææ•°æ®
            analysis = self.analyze_jobs(jobs)

            # 4. å¯è§†åŒ–åˆ†æç»“æœ
            if analysis:
                self.visualize_analysis(analysis, keyword)

            logger.info(f"âœ… çˆ¬å–å®Œæˆ! å…±è·å– {len(jobs)} ä¸ªèŒä½")

        except Exception as e:
            logger.error(f"âŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}")
        finally:
            await self.close()


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ç°ä»£åŒ–æ‹›è˜ä¿¡æ¯çˆ¬è™«ç³»ç»Ÿ")
    print("=" * 50)

    # åˆå§‹åŒ–çˆ¬è™«
    spider = JobSpider()

    # è®¾ç½®æœç´¢å…³é”®è¯
    keywords = ["Pythonå·¥ç¨‹å¸ˆ", "æ•°æ®åˆ†æå¸ˆ", "æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆ"]

    for keyword in keywords:
        print(f"\nğŸ” å¼€å§‹æœç´¢: {keyword}")
        await spider.run_crawler(keyword, max_pages=2)

        # å…³é”®è¯é—´é—´éš”
        await asyncio.sleep(5)

    print("\nğŸ‰ æ‰€æœ‰å…³é”®è¯æœç´¢å®Œæˆ!")
    print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print("- job_data.db: æ‹›è˜æ•°æ®æ•°æ®åº“")
    print("- job_analysis_*.png: æ•°æ®å¯è§†åŒ–å›¾è¡¨")
    print("- job_report_*.md: è¯¦ç»†åˆ†ææŠ¥å‘Š")


if __name__ == "__main__":
    asyncio.run(main())

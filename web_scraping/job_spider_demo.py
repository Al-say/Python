# æ‹›è˜ä¿¡æ¯çˆ¬è™«æ¼”ç¤º
# æ¼”ç¤ºåŸºæœ¬çš„çˆ¬è™«æŠ€æœ¯å’Œæ•°æ®å¤„ç†

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import json
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import sqlite3
from fake_useragent import UserAgent
import re

class SimpleJobSpider:
    """ç®€åŒ–çš„æ‹›è˜ä¿¡æ¯çˆ¬è™«æ¼”ç¤º"""

    def __init__(self):
        self.ua = UserAgent()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })

        # åˆå§‹åŒ–æ•°æ®åº“
        self.init_database()

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        self.conn = sqlite3.connect('job_demo.db')
        cursor = self.conn.cursor()

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS demo_jobs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT,
                company TEXT,
                salary TEXT,
                location TEXT,
                source TEXT,
                crawl_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        self.conn.commit()

    def crawl_zhipin_demo(self, keyword="Python", pages=1):
        """æ¼”ç¤ºçˆ¬å–Bossç›´è˜ï¼ˆæ³¨æ„ï¼šå®é™…ä½¿ç”¨æ—¶éœ€éµå®ˆç½‘ç«™è§„åˆ™ï¼‰"""
        print(f"ğŸ” æ¼”ç¤ºçˆ¬å–Bossç›´è˜ - å…³é”®è¯: {keyword}")

        jobs = []

        for page in range(1, pages + 1):
            try:
                # æ³¨æ„ï¼šè¿™æ˜¯æ¼”ç¤ºURLï¼Œå®é™…çˆ¬å–æ—¶éœ€è¦å¤„ç†åŠ¨æ€åŠ è½½å’Œåçˆ¬è™«
                url = f"https://www.zhipin.com/web/geek/job?query={keyword}&page={page}"
                print(f"è¯·æ±‚é¡µé¢: {url}")

                # ç”±äºBossç›´è˜æœ‰åçˆ¬è™«æªæ–½ï¼Œè¿™é‡Œåªåšè¯·æ±‚æ¼”ç¤º
                response = self.session.get(url, timeout=10)

                if response.status_code == 200:
                    print(f"âœ… é¡µé¢ {page} è¯·æ±‚æˆåŠŸ (çŠ¶æ€ç : {response.status_code})")

                    # è§£æHTMLï¼ˆå®é™…é¡¹ç›®ä¸­éœ€è¦å¤„ç†åŠ¨æ€å†…å®¹ï¼‰
                    soup = BeautifulSoup(response.text, 'html.parser')

                    # è¿™é‡Œæ˜¯æ¼”ç¤ºæ•°æ®ï¼Œå®é™…çˆ¬å–éœ€è¦æ ¹æ®é¡µé¢ç»“æ„è°ƒæ•´é€‰æ‹©å™¨
                    demo_jobs = [
                        {
                            'title': f'Pythonå¼€å‘å·¥ç¨‹å¸ˆ-{page}-{i+1}',
                            'company': f'ç§‘æŠ€å…¬å¸{i+1}',
                            'salary': f'{random.randint(15, 50)}k-{random.randint(20, 80)}k',
                            'location': random.choice(['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³', 'æ­å·', 'å¹¿å·']),
                            'source': 'Bossç›´è˜(æ¼”ç¤º)'
                        } for i in range(5)
                    ]

                    jobs.extend(demo_jobs)

                else:
                    print(f"âŒ é¡µé¢ {page} è¯·æ±‚å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")

            except Exception as e:
                print(f"âŒ çˆ¬å–é¡µé¢ {page} æ—¶å‡ºé”™: {e}")

            # å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(random.uniform(1, 3))

        return jobs

    def crawl_lagou_demo(self, keyword="Python", pages=1):
        """æ¼”ç¤ºçˆ¬å–æ‹‰å‹¾ç½‘"""
        print(f"ğŸ” æ¼”ç¤ºçˆ¬å–æ‹‰å‹¾ç½‘ - å…³é”®è¯: {keyword}")

        jobs = []

        for page in range(1, pages + 1):
            try:
                url = f"https://www.lagou.com/wn/jobs?pn={page}&kd={keyword}"
                print(f"è¯·æ±‚é¡µé¢: {url}")

                response = self.session.get(url, timeout=10)

                if response.status_code == 200:
                    print(f"âœ… é¡µé¢ {page} è¯·æ±‚æˆåŠŸ (çŠ¶æ€ç : {response.status_code})")

                    # æ¼”ç¤ºæ•°æ®
                    demo_jobs = [
                        {
                            'title': f'åç«¯å¼€å‘å·¥ç¨‹å¸ˆ-{page}-{i+1}',
                            'company': f'äº’è”ç½‘å…¬å¸{i+1}',
                            'salary': f'{random.randint(20, 60)}k-{random.randint(30, 100)}k',
                            'location': random.choice(['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³', 'æ­å·', 'æˆéƒ½']),
                            'source': 'æ‹‰å‹¾ç½‘(æ¼”ç¤º)'
                        } for i in range(5)
                    ]

                    jobs.extend(demo_jobs)

                else:
                    print(f"âŒ é¡µé¢ {page} è¯·æ±‚å¤±è´¥ (çŠ¶æ€ç : {response.status_code})")

            except Exception as e:
                print(f"âŒ çˆ¬å–é¡µé¢ {page} æ—¶å‡ºé”™: {e}")

            time.sleep(random.uniform(1, 3))

        return jobs

    def save_jobs_to_db(self, jobs):
        """ä¿å­˜èŒä½åˆ°æ•°æ®åº“"""
        cursor = self.conn.cursor()

        for job in jobs:
            try:
                cursor.execute('''
                    INSERT INTO demo_jobs (title, company, salary, location, source)
                    VALUES (?, ?, ?, ?, ?)
                ''', (job['title'], job['company'], job['salary'], job['location'], job['source']))

            except Exception as e:
                print(f"ä¿å­˜èŒä½å¤±è´¥: {e}")

        self.conn.commit()
        print(f"âœ… å·²ä¿å­˜ {len(jobs)} ä¸ªèŒä½åˆ°æ•°æ®åº“")

    def analyze_and_visualize(self):
        """åˆ†ææ•°æ®å¹¶å¯è§†åŒ–"""
        # ä»æ•°æ®åº“è¯»å–æ•°æ®
        df = pd.read_sql_query("SELECT * FROM demo_jobs", self.conn)

        if df.empty:
            print("âŒ æ²¡æœ‰æ•°æ®å¯åˆ†æ")
            return

        print("\nğŸ“Š æ•°æ®åˆ†æ:")
        print(f"æ€»èŒä½æ•°: {len(df)}")
        print(f"ç‹¬ç‰¹å…¬å¸æ•°: {df['company'].nunique()}")
        print(f"æ•°æ®æºåˆ†å¸ƒ: {df['source'].value_counts().to_dict()}")

        # è–ªèµ„åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰
        def extract_salary_range(salary_str):
            """æå–è–ªèµ„èŒƒå›´"""
            numbers = re.findall(r'\d+', salary_str)
            if len(numbers) >= 2:
                return (int(numbers[0]) + int(numbers[1])) / 2  # å–å¹³å‡å€¼
            return 0

        df['avg_salary'] = df['salary'].apply(extract_salary_range)

        # å¯è§†åŒ–
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('æ‹›è˜æ•°æ®åˆ†ææ¼”ç¤º', fontsize=16)

        # 1. æ•°æ®æºåˆ†å¸ƒ
        source_counts = df['source'].value_counts()
        axes[0, 0].pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%')
        axes[0, 0].set_title('æ•°æ®æºåˆ†å¸ƒ')

        # 2. è–ªèµ„åˆ†å¸ƒ
        if df['avg_salary'].max() > 0:
            axes[0, 1].hist(df['avg_salary'], bins=10, edgecolor='black')
            axes[0, 1].set_title('è–ªèµ„åˆ†å¸ƒ')
            axes[0, 1].set_xlabel('å¹³å‡è–ªèµ„(k)')
            axes[0, 1].set_ylabel('èŒä½æ•°é‡')

        # 3. å·¥ä½œåœ°ç‚¹åˆ†å¸ƒ
        location_counts = df['location'].value_counts()
        axes[1, 0].bar(location_counts.index, location_counts.values)
        axes[1, 0].set_title('å·¥ä½œåœ°ç‚¹åˆ†å¸ƒ')
        axes[1, 0].tick_params(axis='x', rotation=45)

        # 4. å…¬å¸èŒä½æ•°é‡
        company_counts = df['company'].value_counts().head(10)
        axes[1, 1].barh(company_counts.index, company_counts.values)
        axes[1, 1].set_title('å…¬å¸èŒä½æ•°é‡TOP10')

        plt.tight_layout()
        plt.savefig('job_analysis_demo.png', dpi=300, bbox_inches='tight')
        plt.show()

        # ç”Ÿæˆç®€å•æŠ¥å‘Š
        report = f"""
# æ‹›è˜æ•°æ®åˆ†ææŠ¥å‘Šï¼ˆæ¼”ç¤ºï¼‰
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## åŸºæœ¬ç»Ÿè®¡
- æ€»èŒä½æ•°: {len(df)}
- ç‹¬ç‰¹å…¬å¸æ•°: {df['company'].nunique()}
- å¹³å‡è–ªèµ„: {df['avg_salary'].mean():.1f}k

## æ•°æ®æºåˆ†å¸ƒ
{source_counts.to_string()}

## çƒ­é—¨åŸå¸‚
{location_counts.head(5).to_string()}
"""
        with open('job_report_demo.md', 'w', encoding='utf-8') as f:
            f.write(report)

        print("âœ… åˆ†æå®Œæˆï¼ç”Ÿæˆæ–‡ä»¶ï¼šjob_analysis_demo.png, job_report_demo.md")

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()

def main():
    """ä¸»å‡½æ•°æ¼”ç¤º"""
    print("ğŸ¯ æ‹›è˜ä¿¡æ¯çˆ¬è™«æ¼”ç¤ºç³»ç»Ÿ")
    print("=" * 50)
    print("âš ï¸  æ³¨æ„ï¼šæœ¬æ¼”ç¤ºä»…ç”¨äºå­¦ä¹ ç›®çš„")
    print("âš ï¸  å®é™…çˆ¬å–æ—¶è¯·éµå®ˆç½‘ç«™robots.txtå’Œä½¿ç”¨æ¡æ¬¾")
    print("âš ï¸  å»ºè®®ä½¿ç”¨å®˜æ–¹APIæˆ–åˆä½œæ¥å£")
    print("=" * 50)

    spider = SimpleJobSpider()

    try:
        # 1. çˆ¬å–Bossç›´è˜æ¼”ç¤ºæ•°æ®
        print("\n1ï¸âƒ£ çˆ¬å–Bossç›´è˜æ¼”ç¤ºæ•°æ®...")
        boss_jobs = spider.crawl_zhipin_demo("Python", pages=2)

        # 2. çˆ¬å–æ‹‰å‹¾ç½‘æ¼”ç¤ºæ•°æ®
        print("\n2ï¸âƒ£ çˆ¬å–æ‹‰å‹¾ç½‘æ¼”ç¤ºæ•°æ®...")
        lagou_jobs = spider.crawl_lagou_demo("Python", pages=2)

        # 3. åˆå¹¶æ•°æ®
        all_jobs = boss_jobs + lagou_jobs
        print(f"\nğŸ“Š å…±è·å– {len(all_jobs)} ä¸ªæ¼”ç¤ºèŒä½")

        # 4. ä¿å­˜åˆ°æ•°æ®åº“
        print("\n3ï¸âƒ£ ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“...")
        spider.save_jobs_to_db(all_jobs)

        # 5. åˆ†æå’Œå¯è§†åŒ–
        print("\n4ï¸âƒ£ åˆ†ææ•°æ®å¹¶ç”Ÿæˆå¯è§†åŒ–...")
        spider.analyze_and_visualize()

        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("- job_demo.db: æ¼”ç¤ºæ•°æ®åº“")
        print("- job_analysis_demo.png: æ•°æ®å¯è§†åŒ–å›¾è¡¨")
        print("- job_report_demo.md: åˆ†ææŠ¥å‘Š")

    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    finally:
        spider.close()

if __name__ == "__main__":
    main()
